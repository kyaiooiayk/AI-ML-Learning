{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat? Vanilla LSTMs\\n\\nA simple LSTM configuration is the Vanilla LSTM. It is named Vanilla in this book to di↵erentiate it from deeper \\nLSTMs and the suite of more elaborate configurations. It is the LSTM architecture defined in the original 1997 \\nLSTM paper and the architecture that will give good results on most small sequence prediction problems\\n \\nReference: Long short-term memory networks with python, Jason Brownlee\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What? Vanilla LSTMs\n",
    "\n",
    "A simple LSTM configuration is the Vanilla LSTM. It is named Vanilla in this book to di↵erentiate it from deeper \n",
    "LSTMs and the suite of more elaborate configurations. It is the LSTM architecture defined in the original 1997 \n",
    "LSTM paper and the architecture that will give good results on most small sequence prediction problems\n",
    " \n",
    "https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echo Sequence Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGiven a sequence of random integers as input output the value of a random integer at a specific time input \\nstep that is not specified to the model. For example, given the input sequence of random integers [5, 3, 2] \\nand the chosen time step was the second value, then the expected output is 3. Technically, this is a sequence\\nclassification problem; it is formulated as a many-to-one prediction problem, where there are multiple input \\ntime steps and one output time step at the end of the sequence.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given a sequence of random integers as input output the value of a random integer at a specific time input \n",
    "step that is not specified to the model. For example, given the input sequence of random integers [5, 3, 2] \n",
    "and the chosen time step was the second value, then the expected output is 3. Technically, this is a sequence\n",
    "classification problem; it is formulated as a many-to-one prediction problem, where there are multiple input \n",
    "time steps and one output time step at the end of the sequence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Random Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe can generate random integers in Python using the randint() function that takes two parameters indicating the \\nrange of integers from which to draw values. In this lesson, we will define the problem as having integer values\\nbetween 0 and 99 with 100 unique values.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can generate random integers in Python using the randint() function that takes two parameters indicating the \n",
    "range of integers from which to draw values. In this lesson, we will define the problem as having integer values\n",
    "between 0 and 99 with 100 unique values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of random numbers in [0, n_features)\n",
    "def generate_sequence(length, n_features):\n",
    "    return [randint(0, n_features-1) for _ in range(length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe need to transform them into a format that is suitable for training an LSTM network. In this case, we can use a \\none hot encoding of the integer values where each value is represented by a 100 element binary vector that is all \\n0 values except the index of the integer, which is marked 1.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We need to transform them into a format that is suitable for training an LSTM network. In this case, we can use a \n",
    "one hot encoding of the integer values where each value is represented by a 100 element binary vector that is all \n",
    "0 values except the index of the integer, which is marked 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode sequence\n",
    "def one_hot_encode(sequence, n_features):\n",
    "    encoding = list()\n",
    "    for value in sequence:\n",
    "        vector = [0 for _ in range(n_features)]\n",
    "        vector[value] = 1\n",
    "        encoding.append(vector)\n",
    "    return array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe can tie all of this together. Below is the complete code listing for generating a sequence of 25 random \\nintegers and encoding each integer as a binary vector.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can tie all of this together. Below is the complete code listing for generating a sequence of 25 random \n",
    "integers and encoding each integer as a binary vector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 12, 16, 79, 5, 38, 83, 4, 53, 50, 27, 33, 25, 23, 84, 9, 4, 71, 12, 67, 68, 36, 84, 29, 38]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[6, 12, 16, 79, 5, 38, 83, 4, 53, 50, 27, 33, 25, 23, 84, 9, 4, 71, 12, 67, 68, 36, 84, 29, 38]\n"
     ]
    }
   ],
   "source": [
    "# generate random sequence\n",
    "sequence = generate_sequence(25, 100)\n",
    "print(sequence)\n",
    "\n",
    "# one hot encode\n",
    "encoded = one_hot_encode(sequence, 100)\n",
    "print(encoded)\n",
    "\n",
    "# one hot decode\n",
    "decoded = one_hot_decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe final step is to reshape the one hot encoded sequences into a format that can be used as input to the LSTM. \\nThis involves reshaping the encoded sequence to have n time steps and k features, where n is the number of \\nintegers in the generated sequence and k is the set of possible integers at each time step (e.g. 100)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The final step is to reshape the one hot encoded sequences into a format that can be used as input to the LSTM. \n",
    "This involves reshaping the encoded sequence to have n time steps and k features, where n is the number of \n",
    "integers in the generated sequence and k is the set of possible integers at each time step (e.g. 100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one example for an lstm\n",
    "def generate_example(length, n_features, out_index):\n",
    "    # generate sequence\n",
    "    sequence = generate_sequence(length, n_features)\n",
    "    # one hot encode\n",
    "    encoded = one_hot_encode(sequence, n_features)\n",
    "    # reshape sequence to be 3D\n",
    "    X = encoded.reshape((1, length, n_features))\n",
    "    # select output\n",
    "    y = encoded[out_index].reshape(1, n_features)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked out example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe can put all of this together and test the generation of one example ready for fitting or evaluating an LSTM.\\nRunning the code generates one encoded sequence and prints out the shape of the input and output components of \\nthe sequence for the LSTM.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can put all of this together and test the generation of one example ready for fitting or evaluating an LSTM.\n",
    "Running the code generates one encoded sequence and prints out the shape of the input and output components of \n",
    "the sequence for the LSTM.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25, 100)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_example(25, 100, 2)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo keep the model small and ensure it is fit in a reasonable time, we will greatly simplify the problem by \\nreducing the sequence length to 5 integers and the number of features to 10 (e.g. 0-9). The model must specify \\nthe expected dimensionality of the input data. In this case, in terms of time steps (5) and features (10). We \\nwill use a single hidden layer LSTM with 25 memory units, chosen with a little trial and error. The output layer \\nis a fully connected layer (Dense) with 10 neurons for the 10 possible integers that may be output. A softmax \\nactivation function is used on the output layer to allow the network to learn and output the distribution over\\nthe possible output values. mThe network will use the log loss function while training, suitable for multiclass \\nclassification problems, and the efficient Adam optimization algorithm. The accuracy metric will be reported each \\ntraining epoch to give an idea of the skill of the model in addition to the loss\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To keep the model small and ensure it is fit in a reasonable time, we will greatly simplify the problem by \n",
    "reducing the sequence length to 5 integers and the number of features to 10 (e.g. 0-9). The model must specify \n",
    "the expected dimensionality of the input data. In this case, in terms of time steps (5) and features (10). We \n",
    "will use a single hidden layer LSTM with 25 memory units, chosen with a little trial and error. The output layer \n",
    "is a fully connected layer (Dense) with 10 neurons for the 10 possible integers that may be output. A softmax \n",
    "activation function is used on the output layer to allow the network to learn and output the distribution over\n",
    "the possible output values. mThe network will use the log loss function while training, suitable for multiclass \n",
    "classification problems, and the efficient Adam optimization algorithm. The accuracy metric will be reported each \n",
    "training epoch to give an idea of the skill of the model in addition to the loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 25)                3600      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 3,860\n",
      "Trainable params: 3,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "length = 5\n",
    "n_features = 10\n",
    "out_index = 2\n",
    "model = Sequential()\n",
    "model.add(LSTM(25, input_shape=(length, n_features)))\n",
    "model.add(Dense(n_features, activation= \"softmax\"))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer= \"adam\" , metrics=[\"acc\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe number of epochs is the number of iterations of generating samples and essentially the batch size is 1 sample.\\nBelow is an example of fitting the model for 10,000 epochs found with a little trial and error.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The number of epochs is the number of iterations of generating samples and essentially the batch size is 1 sample.\n",
    "Below is an example of fitting the model for 10,000 epochs found with a little trial and error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "for i in range(3000):    \n",
    "    X, y = generate_example(length, n_features, out_index)\n",
    "    model.fit(X, y, epochs=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Once the model is fit, we can estimate the skill of the model when classifying new random sequences. We can do \n",
    "this by simply making predictions on 100 randomly generated sequences and counting the number of correct \n",
    "predictions made.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "correct = 0\n",
    "for i in range(1000):\n",
    "    X, y = generate_example(length, n_features, out_index)\n",
    "    yhat = model.predict(X)\n",
    "    if one_hot_decode(yhat) == one_hot_decode(y):\n",
    "        correct += 1\n",
    "print(\"Accuracy: %f\"  % ((correct/100)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions With the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finally, we can use the fit model to make predictions on new randomly generated sequences. For this problem,\n",
    "this is much the same as the case of evaluating the model. Because this is more of a user-facing activity, we\n",
    "can decode the whole sequence, expected output, and prediction and print them on the screen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on new data\n",
    "X, y = generate_example(length, n_features, out_index)\n",
    "yhat = model.predict(X)\n",
    "print(\"Sequence: %s\"  % [one_hot_decode(x) for x in X])\n",
    "print(\"Expected: %s\"  % one_hot_decode(y))\n",
    "print(\"Predicted: %s\"  % one_hot_decode(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I have used 1000 instead of 10k, as it was taking too lon\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
